{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to Implement VPG for cartpole:\n",
    "\n",
    "1. Create a policy network for cartpole. Need the input size and ranges, output size and ranges, and network architecture.\n",
    "2. Create value network. Can make this as a separate head of the policy network, so that they share the representation, but one head outputs the value and the other the policy.\n",
    "3. Initialize the policy network to some sensible parameters.\n",
    "4. Create the training loop:\n",
    "    1. Run the set policy for $k$ encounters, and compute the return to go $\\hat{R_t}$ for each encounter. (why do we calculate return to go and not the total return?)\n",
    "    2. Compute the advantage estimates $\\tilde{A_t}$. Advantage $\\tilde{A_t} = \\hat{R_t} - V_{\\phi_k}$.\n",
    "    3. Estimate the policy gradient as \n",
    "    $$\n",
    "    \\hat{g} = \\sum_{t=0}^{T-1} \\log \\pi(a_t | s_t, \\theta) \\left(\\sum_{t' = t}^{T-1} r_{t'} - V_{\\phi_k}(s_t)\\right)\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and inspect cartpole env\n",
    "\n",
    "The specification of the cartpole environment is: \n",
    "\n",
    "    Box(4):\n",
    "        Num\tObservation                 Min         Max\n",
    "        0\tCart Position             -4.8            4.8\n",
    "        1\tCart Velocity             -Inf            Inf\n",
    "        2\tPole Angle                 -24 deg        24 deg\n",
    "        3\tPole Velocity At Tip      -Inf            Inf\n",
    "        \n",
    "    Actions:\n",
    "        Type: Discrete(2)\n",
    "        Num\tAction\n",
    "        0\tPush cart to the left\n",
    "        1\tPush cart to the right\n",
    "        \n",
    "    Reward:\n",
    "        Reward is 1 for every step taken, including the termination step.\n",
    "        \n",
    "    Starting State:\n",
    "        All observations are assigned a uniform random value in [-0.05..0.05].\n",
    "        \n",
    "    Episode Termination:\n",
    "        Pole Angle is more than 12 degrees\n",
    "        Cart Position is more than 2.4 (center of the cart reaches the edge of the display)\n",
    "        Episode length is greater than 200\n",
    "        \n",
    "    Solved Requirements:\n",
    "        Considered solved when the average reward is greater than or equal to 195.0 over 100 consecutive trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Actor Critic Network from Environment Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_size = env.observation_space.shape[0]\n",
    "out_size = env.action_space.n\n",
    "print(f'in={in_size}, out={out_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a neural network that takes in the observations and outputs (1) the value of the state and (2) the policy values for each action at that state.\n",
    "\n",
    "The neural network has two output heads, one for the policy and one for the value function. It has one hidden layer to encode the representation of the state:\n",
    "\n",
    "in -> hidden(16) \\\n",
    "-> value(16) -> value-out\\\n",
    "-> $\\pi$(16) -> $\\pi$-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc = nn.Linear(in_size, 16)\n",
    "        self.fc1 = nn.Linear(16, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.pi_out = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Output value of observation, and policy action.\n",
    "        \n",
    "        The output of policy is interpreted as the \n",
    "        probability that the action at state x is 0\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc(x))\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "        policy = torch.sigmoid(self.pi_out(x))\n",
    "        \n",
    "        return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ActorCritic(in_size, out_size)\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Collect rewards from the environment for each episode.\n",
    "2. Collect a batch of episodes.\n",
    "3. Act in the environment with neural network.\n",
    "4. Write loss function for policy gradient.\n",
    "5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "\n",
    "def calc_loss(n):\n",
    "    summations = []\n",
    "    \n",
    "    for i_episode in range(n):\n",
    "        rewards = []\n",
    "        log_pis = []\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        t = 0\n",
    "        while not done:\n",
    "            prob_1 = net(torch.Tensor(observation))\n",
    "            \n",
    "            action = 1 if random.random() <= prob_1 else 0\n",
    "            \n",
    "            log_pis.append(torch.log(prob_1 if action == 1 else 1 - prob_1))\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            \n",
    "            rewards.append(reward)\n",
    "            \n",
    "            t += 1\n",
    "\n",
    "        episode_summation = 0\n",
    "        #print(np.exp(log_pis))\n",
    "        for i in range(len(rewards)):\n",
    "            log_pi = log_pis[i]\n",
    "            inner_sum = 0\n",
    "            \n",
    "            for j in range(i, len(rewards)):\n",
    "                inner_sum += rewards[j]\n",
    "            episode_summation += log_pi*inner_sum\n",
    "        summations.append(episode_summation)\n",
    "        \n",
    "    return sum(summations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(torch.log(torch.Tensor([0.7])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = ActorCritic(in_size, out_size)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "current_loss = 0\n",
    "for x in range(300):\n",
    "    \n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    loss = -calc_loss(5)\n",
    "    loss.backward()\n",
    "    current_loss += loss\n",
    "    optimizer.step()    # Does the update\n",
    "    if x % 100 == 0:\n",
    "        print(x, current_loss/100.0)\n",
    "        current_loss = 0\n",
    "        \n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "for i_episode in range(3):\n",
    "    observation = env.reset()\n",
    "    for t in range(1000):\n",
    "        env.render()\n",
    "        policy = net(torch.Tensor(observation))\n",
    "        action = 1 if random.random() <= policy else 0\n",
    "        observation, reward, done, info = env.step(action)\n",
    "#         if t == 99:\n",
    "#             print(\"FK not enough T\")\n",
    "#         if done:\n",
    "#             break\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(100):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "    print(env.action_space.sample())\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
