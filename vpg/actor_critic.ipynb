{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import Module, Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(Module):\n",
    "    \n",
    "    def __init__(self, in_size, out_size, num_layers=2, num_hidden=64):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc = [Linear(in_size, num_hidden)]\n",
    "        \n",
    "        for _ in range(num_layers - 1):\n",
    "            self.fc.append(Linear(num_hidden, num_hidden))\n",
    "        \n",
    "        self.val = Linear(num_hidden, 1)\n",
    "        self.pi = Linear(num_hidden, out_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.fc:\n",
    "            x = F.relu(layer(x))\n",
    "            \n",
    "        val = self.val(x)\n",
    "        log_pi = F.log_softmax(self.pi(x), dim=0)\n",
    "        \n",
    "        return val, log_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Log-prob from Logits\n",
    "\n",
    "$$\\log \\pi(a_0) = \\log (\\frac{e^{x_0}}{e^{x_0} + e^{x_1}})$$\n",
    "$$= x_0 - \\log(e^{x_0} + e^{x_1})$$\n",
    "$$= (x_0 - x^*) - \\log(e^{x_0 - x^*} + e^{x_1 - x^*})$$\n",
    "$$x^* = \\max (x_0, x_1)$$\n",
    "\n",
    "$$\\log \\pi(a_i) = x_i - \\text{logsumexp}(x_1, \\ldots, x_n)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_loss(trajectory, advantage, model):\n",
    "    \"\"\"\n",
    "    A trajectory is a sequence of the following:\n",
    "    \n",
    "    (x_t, a_t, r_t)\n",
    "    \n",
    "    x_t: The observation at time t\n",
    "    a_t: The action taken at time t, given x_t.\n",
    "    r_t: The reward received for taking action a_t at x_t.\n",
    "    \"\"\"\n",
    "    n = len(trajectory)\n",
    "    pi_loss = 0\n",
    "    \n",
    "    for t in range(n):\n",
    "        adv_t = advantage[t]\n",
    "        x_t, a_t, r_t = trajectory[t]\n",
    "        _, log_pi_xt = model(x_t)\n",
    "        \n",
    "        pi_loss -= log_pi_xt[a_t] * adv_t\n",
    "        \n",
    "    pi_loss /= n\n",
    "    \n",
    "    return pi_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trajectory_return(trajectory, discount):\n",
    "    n = len(trajectory)\n",
    "    rw_to_go = {}\n",
    "    rw_sum = 0\n",
    "    \n",
    "    # Calculate suffix-sums of reward\n",
    "    for t in reversed(range(n)):\n",
    "        _, _, r_t = trajectory[t]\n",
    "        rw_sum = r_t + discount * rw_sum\n",
    "        rw_to_go[t] = rw_sum\n",
    "        \n",
    "    return rw_to_go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_loss(trajectory, discount, model):\n",
    "    n = len(trajectory)\n",
    "    rw_to_go = trajectory_return(trajectory, discount)\n",
    "    ret = torch.empty((n,1))\n",
    "    \n",
    "    for t in range(n):\n",
    "        ret[t] = rw_to_go[t]\n",
    "    \n",
    "    # Calculate advantage values with gradient\n",
    "    xs, _, _ = zip(*trajectory)\n",
    "    x_tensor = torch.stack(xs)\n",
    "    vals, _ = model(x_tensor)\n",
    "    \n",
    "    return F.mse_loss(vals, ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advantage_function(trajectory, discount, model):\n",
    "    \"\"\"\n",
    "    Create a dictionary with the advantage at each\n",
    "    timestep.\n",
    "    \"\"\"\n",
    "    n = len(trajectory)\n",
    "    advantage = []\n",
    "    rw_to_go = {}\n",
    "    rw_sum = 0\n",
    "    \n",
    "    # Calculate suffix-sums of reward\n",
    "    for t in reversed(range(n)):\n",
    "        _, _, r_t = trajectory[t]\n",
    "        rw_sum = r_t + discount * rw_sum\n",
    "        rw_to_go[t] = rw_sum\n",
    "        \n",
    "    # Calculate advantage\n",
    "    for t in range(n):\n",
    "        x_t, _, _ = trajectory[t]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_xt, _ = model(x_t)\n",
    "            advantage.append(rw_to_go[t] - val_xt)\n",
    "    \n",
    "    return torch.stack(advantage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_to_tensor(obs):\n",
    "    obs = obs.astype('float32')\n",
    "    obs = torch.from_numpy(obs)\n",
    "    \n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_act(log_probs):\n",
    "    u = random()\n",
    "    p = torch.exp(log_probs)\n",
    "    \n",
    "    cum_p = 0\n",
    "    \n",
    "    for i in range(len(log_probs)):\n",
    "        cum_p += p[i]\n",
    "        \n",
    "        if u <= cum_p:\n",
    "            return i\n",
    "    \n",
    "    # Return last action in case there is a\n",
    "    # rounding error and cum_p doesn't go to 1\n",
    "    return len(log_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model, lr):\n",
    "    return torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(env, model, optimizer, c, discount, n_episodes):\n",
    "    # Initialize loss and zero out gradients on parameters\n",
    "    loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Collect trajectories for n_episodes\n",
    "    for _ in range(n_episodes):\n",
    "        trajectory = []\n",
    "        \n",
    "        x_t = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                x_t = obs_to_tensor(x_t)\n",
    "                _, log_pi_xt = model(x_t)\n",
    "            \n",
    "            a_t = sample_act(log_pi_xt)\n",
    "            x_tp1, r_t, done, _ = env.step(a_t)\n",
    "            \n",
    "            trajectory.append((x_t, a_t, r_t))\n",
    "            x_t = x_tp1\n",
    "            \n",
    "            # Note: We can collect the log_pi at every\n",
    "            # step here, and the value at every state, \n",
    "            # so that we can combined them into the \n",
    "            # loss after we're done\n",
    "        \n",
    "        val_loss = value_loss(trajectory, discount, model)\n",
    "        advantage = advantage_function(trajectory, discount, model)\n",
    "        pi_loss = policy_loss(trajectory, advantage, model)\n",
    "        \n",
    "        loss += pi_loss + c*val_loss\n",
    "        \n",
    "        \n",
    "    # Perform gradient step\n",
    "    loss /= n_episodes\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cartpole(\n",
    "    n_epochs, \n",
    "    n_episodes,\n",
    "    print_freq=100,\n",
    "    discount=.99,\n",
    "    lr=.01,\n",
    "    c=.01\n",
    "):\n",
    "    env = gym.make('CartPole-v0')\n",
    "    in_size = env.observation_space.shape[0]\n",
    "    out_size = env.action_space.n\n",
    "    \n",
    "    model = ActorCritic(in_size, out_size)\n",
    "    optimizer = create_optimizer(model, lr)\n",
    "    \n",
    "    loss = 0\n",
    "    for t in tqdm(range(n_epochs)):\n",
    "        loss += train_step(env, model, optimizer, c, discount, n_episodes)\n",
    "        \n",
    "        if (t + 1) % print_freq == 0:\n",
    "            print(loss / print_freq)\n",
    "            loss = 0\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_cartpole(\n",
    "    model,\n",
    "    n_episodes,\n",
    "    step_len=.02\n",
    "):\n",
    "    env = gym.make('CartPole-v0')\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        x_t = env.reset()\n",
    "        done = False\n",
    "        t = 0\n",
    "        \n",
    "        while not done:\n",
    "            env.render()\n",
    "            with torch.no_grad():\n",
    "                x_t = obs_to_tensor(x_t)\n",
    "                _, log_pi_xt = model(x_t)\n",
    "            \n",
    "            a_t = sample_act(log_pi_xt)\n",
    "            x_t, _, done, _ = env.step(a_t)\n",
    "            t += 1\n",
    "        print(f'ep_len: {t}')\n",
    "    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = train_cartpole(500, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_cartpole(model, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Algorithm Parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "discount=.99\n",
    "lr=.01\n",
    "c=.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment and model definitions\n",
    "env = gym.make('CartPole-v0')\n",
    "in_size = env.observation_space.shape[0]\n",
    "out_size = env.action_space.n\n",
    "\n",
    "model = ActorCritic(in_size, out_size, num_layers=1, num_hidden=16)\n",
    "optimizer = create_optimizer(model, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model output sanity check\n",
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obs_32 = obs.astype('float32')\n",
    "x = torch.from_numpy(obs_32)\n",
    "with torch.no_grad():\n",
    "    val, pi = model(x)\n",
    "(val, pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step(env, model, optimizer, c, discount, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
